{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader for CUB Birds, Stanford Dogs and Foodx dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision as tv\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dataloader for CUB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUBDataset(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Dataset class for CUB Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_root_path, caption_root_path=None, split=\"train\", *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_root_path:      path to dir containing images and lists folders\n",
    "            caption_root_path:    path to dir containing captions\n",
    "            split:          train / test\n",
    "            *args:\n",
    "            **kwargs:\n",
    "        \"\"\"\n",
    "        image_info = self.get_file_content(f\"{image_root_path}/images.txt\")\n",
    "        self.image_id_to_name = {y[0]: y[1] for y in [x.strip().split(\" \") for x in image_info]}\n",
    "        split_info = self.get_file_content(f\"{image_root_path}/train_test_split.txt\")\n",
    "        self.split_info = {self.image_id_to_name[y[0]]: y[1] for y in [x.strip().split(\" \") for x in split_info]}\n",
    "        self.split = \"1\" if split == \"train\" else \"0\"\n",
    "        self.caption_root_path = caption_root_path\n",
    "\n",
    "        super(CUBDataset, self).__init__(root=f\"{image_root_path}/images\", is_valid_file=self.is_valid_file,\n",
    "                                         *args, **kwargs)\n",
    "\n",
    "    def is_valid_file(self, x):\n",
    "        return self.split_info[(x[len(self.root) + 1:])] == self.split\n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_content(file_path):\n",
    "        with open(file_path) as fo:\n",
    "            content = fo.readlines()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/datasets/CUB/CUB_200_2011/\"\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "# write data transform here as per the requirement\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "train_dataset_cub = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"train\")\n",
    "test_dataset_cub = CUBDataset(image_root_path=f\"{data_root}\", transform=data_transform, split=\"test\")\n",
    "\n",
    "\n",
    "# load in into the torch dataloader to get variable batch size, shuffle \n",
    "train_loader_cub = torch.utils.data.DataLoader(train_dataset_cub, batch_size=16, drop_last=True, shuffle=True)\n",
    "test_loader_cub = torch.utils.data.DataLoader(test_dataset_cub, batch_size=16, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5994, 5794)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset_cub), len(test_dataset_cub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374, 363)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_cub), len(test_loader_cub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "labels tensor([ 42,  85,  69,  43,  44,  66,  94, 109,  30,  85,  64,  82,   8,  58,\n",
      "        187,  46])\n",
      "label() len 16\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader_cub):\n",
    "    print(inputs.shape)\n",
    "    print( \"labels\",labels)\n",
    "    print(\"label() len\",len(labels))\n",
    "    print('='*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "labels tensor([ 73, 157, 186, 111,  14, 109, 172,  85, 199, 149, 146, 120,   2,  32,\n",
      "        160,  23])\n",
      "label() len 16\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader_cub):\n",
    "    print(inputs.shape)\n",
    "    print( \"labels\",labels)\n",
    "    print(\"label() len\",len(labels))\n",
    "    print('='*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "num_epochs=20 \n",
    "learning_rate=0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(vgg.parameters(), lr=learning_rate)\n",
    "optimizer = optim.SGD(vggm.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.to(device)\n",
    "# change the number of classes \n",
    "vggm16.classifier[6].out_features = 200\n",
    "# freeze convolution weights\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def fit(model, train_dataloader):\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        data, target = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_running_loss += loss.item()\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        train_running_correct += (preds == target).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = train_running_loss/len(train_dataloader.dataset)\n",
    "    train_accuracy = 100. * train_running_correct/len(train_dataloader.dataset)\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}')\n",
    "    \n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation function\n",
    "def validate(model, test_dataloader):\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    for int, data in enumerate(test_dataloader):\n",
    "        data, target = data[0].to(device), data[1].to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        val_running_loss += loss.item()\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        val_running_correct += (preds == target).sum().item()\n",
    "    \n",
    "    val_loss = val_running_loss/len(test_dataloader.dataset)\n",
    "    val_accuracy = 100. * val_running_correct/len(test_dataloader.dataset)\n",
    "    \n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss , train_accuracy = [], []\n",
    "val_loss , val_accuracy = [], []\n",
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    train_epoch_loss, train_epoch_accuracy = fit(vgg16, train_loader_cub)\n",
    "    val_epoch_loss, val_epoch_accuracy = validate(vgg16,test_loader_cub)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_accuracy.append(train_epoch_accuracy)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "end = time.time()\n",
    "print((end-start)/60, 'minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
